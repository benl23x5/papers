%!TEX root = ../Main.tex
\section{Introdution}

The functional language ecosystem is blessed with a multitude of libraries for writing streaming data flow programs. Stand out examples for Haskell include iteratee \cite{Kiselyov:iteratee}, enumerator~\cite{hackage:enumerator}, conduit~\cite{hackage:conduit} and pipes~\cite{hackage:pipes}. For Scala we have Scalaz-Streams~\CITE and Akka~\CITE.

These libraries are typically used to deal with data sets that do not fit in main memory, as the constant space guarantee ensures that the program will run to completion without suffering an out-of-memory error. However, current computing platforms use multi-core processors, the programming models provided by such streaming libraries do not also provide a notion of \emph{parallelism} to help deal with the implied amount of data. They also lack support for branching data flows where produced streams can be consumed by several consumers without the programmer needing to had fuse them.

We provide several techniques that increase the scope of programs that can be written in such libraries. Our target applications concern \emph{medium data}, meaning data that is large enough that it does not fit in the main memory of a normal desktop machine, but not so large that we require a cluster of multiple physical machines. For a lesser amount of data one could simply load the data into main memory and use an in-memory array library such as CITE or CITE. For greater data one needs to turn to a distributed system such as Hadoop~\cite{Shvachko:Hadoop}, Spark and deal with the unreliable network and lack of shared memory. Repa Flow targets the sweet middle ground.

We make the following contributions:

\begin{itemize}
\item Our parallel data flows consist of a bundle of streams, where each stream can process a separate partition of a data set on a separate processor core.

\item Our API uses polarised flow endpoints (@Sources@ and @Sinks@) to ensure that programs run in constant space. We demonstrate how this technique can be extended to branching data flows, where produced flows are consumed by multiple consumers.

\item The data processed by our streams is chunked so that each operation processes several elements at a time. We show how to design the core API in a generic fashion so that chunk-at-a-time operators can interoperate smoothly with element-at-a-time operators.

\item We show how to use Continuation Passing Style to provoke the Glasgow Haskell Compiler into applying stream fusion across chunks processed by independent flow operators. For example, the map-map fusion on flows arises naturally from map-map fusion rule on chunks (arrays) of elements.
\end{itemize}

Our work is embodied in Repa Flow, which is available on Hackage. \TODO{Specify the relationship to previous work on Repa} \cite{Lippmeier:Guiding, Lippmeier:DataFlow}. This is a new layer on the original delayed arrays of our original Repa library.

We use stream processing to deal with large data sets that do not fit in memory,
rather than to process unbounded streams in real time, such as click streams generated from websites. These latter applications are the domain of synchronous data flow languages such as Lucy~\cite{Mandel:Lucy}, and reactive stream processing systems such as Heron~\cite{Kulkarn:Heron} and S4~\cite{Neumeyer:S4}. Our system ensures that the programs run in constant space, without requiring buffering or backpressure as in Akka. Stream programs fused into nested loops that read the input data from the source files, process it, and write it to the result files immediately. The only intermediate space required is for aggregators, for example when counting the number of occurrences of each word in a stream of text we must retain a map of words to their number of occurrences. No extra extra buffering is required to match differing rates of production and consumption. CITE Brook, StreamIt.

