%!TEX root = ../Main.tex
\section{Introduction}
\label{s:Introduction}
A collection-oriented programming model is ideal for expressing data-parallel computations, as it exposes the communication patterns to the compiler without requiring complicated loop analysis.
It is essential, however, that the compiler combines these operations into a small number of efficient loops, as a naive translation leads to high memory traffic and poor performance.
This is a well known problem, and techniques to fuse subsequent array operations together have been presented in~\cite{gill1993short, coutts2007streamfusion, keller2010repa}, to name just a few.
This type of fusion alone is not generally sufficient to minimise memory traffic.
As shown in Megiddo~\cite{megiddo1998optimal} and Darte~\cite{darte2002contraction}, Integer Linear Programming can be used to find good clusterings.
Unfortunately, they cannot handle operations like filter, where the output size differs from the input size.
We present a technique that can handle both multi-loop fragments as well as size-altering operations. 

To compile the clusters found by our clustering technique into sequential loops, we use data flow fusion~\cite{lippmeier2013flow}.
It improved on existing array fusion approaches~\cite{coutts2007streamfusion, keller2010repa} as it guarantees fusion into a single loop for programs that operate on the same size input data and contain no fusion-preventing dependencies between operators. 

To see the effect of clustering, consider the following program:
\begin{code}
 normalize2 :: Array Int -> (Array Int, Array Int)
 normalize2 xs
  = let sum1 = fold   (+)  0   xs
        gts  = filter (>   0)  xs
        sum2 = fold   (+)  0   gts
        ys1  = map    (/ sum1) xs
        ys2  = map    (/ sum2) xs
    in (ys1, ys2)
\end{code}

The function @normalize2@ computes two sums, one of all the elements of @xs@, the other of only elements greater than zero. Since we need to fully evalute the sums to proceed with the maps, it is clear that we need at least two separate loops. These folds are examples of fusion-preventing dependencies, as fold cannot be fused with subsequent operations. Figure~\ref{f:normalize2-clusterings} shows the data-flow graph of @normalize2@. In the leftmost diagram we can see the effect of applying stream fusion to the program: we end up with four loops (denoted by the dotted lines): only the filter operation is combined with the subsequent fold. The best existing ILP approach results in the rightmost graph: it combines the @sum1@ fold and @sum2@ filter in one loop, but requires an extra loop for the fold operation which consumes the filter output, since it cannot fuse filter operations. Our approach, in the middle, produces the optimal solution in this case: one loop for the sums, another for the maps.

Our contributions are as follows:
\begin{itemize}
\item   
We extend prior work by Megiddo~\cite{megiddo1998optimal} and Darte~\cite{darte2002contraction}, with support for size changing operators.
Size changing operators can be clustered with operations on both their source array and output array, and compiled naturally with data-flow fusion (\S\ref{s:ILP}).

\item
We present a simplification to constraint generation that is also applicable to some ILP formulations such as Megiddo's:
constraints between two nodes need not be generated if there is a fusion-preventing path between the two (\S\ref{s:OptimisedConstraints}).

\item
Our constraint system encodes a total ordering on the cost of clusterings, expressed using weights on the integer linear program.
For example, we encode that memory traffic is more expensive than loop overheads, so given a choice between the two, memory traffic will be reduced (\S\ref{s:ObjectiveFunction}).

\item
We present benchmarks of our algorithm applied to several common programming patterns.
Our algorithm is complete and yields good results in practice, though an optimal solution is uncomputable in general (\S\ref{s:Benchmarks}).
\end{itemize}

An implementation of our clustering algorithm is available at \url{https://github.com/amosr/clustering}.

