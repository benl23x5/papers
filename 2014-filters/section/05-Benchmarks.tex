%!TEX root = ../Main.tex
\section{Benchmarks}
\label{s:Benchmarks}

\begin{figure*}
$$\begin{array}{c}

\begin{tabular}{lrrrrrrrr}
                & \multicolumn{2}{c}{Unfused}         & \multicolumn{2}{c}{Stream}
                & \multicolumn{2}{c}{Megiddo} &\multicolumn{2}{c}{\textbf{Ours}} \\
                & Time & Loops   & Time & Loops      & Time & Loops & Time & Loops   \\
\hline
Normalize2      & 1.88s & 5      & 1.64s & 4          & 1.82s & 3  & \textbf{1.59s} & \textbf{2}\\
Closest points  & 3.83s & 6      & 3.33s & 5          & 2.92s & 3  & \textbf{2.92s} & \textbf{3}\\
QuadTree        & 5.22s & 8      & 5.22s & 8          & 4.72s & 2  & \textbf{4.72s} & \textbf{2}\\
\end{tabular}

\end{array}$$
\caption{Benchmark results}
\label{f:BenchResults}
\end{figure*}

This section discusses three representative benchmarks, and gives the full ILP program of the first. These benchmarks highlight the main differences between our fusion mechanism and related work. The runtimes of each benchmark are summarized in Figure~\ref{f:BenchResults}. We report times for: the unfused case where each operator is assigned to its own cluster; the clustering implied by stream fusion~\cite{coutts2007streamfusion}; the clustering chosen by Megiddo~\cite{megiddo1998optimal}, and the clustering chosen by our system. 

For each benchmark we report the runtimes of hand-fused C code based on the clustering determined by each algorithm. Although we also have an implementation of our Data Flow Fusion system in terms of a GHC plugin~\cite{lippmeier2013flow}, we report on hand-fused C code to provide a fair comparison to related work. As mentioned in~\cite{lippmeier2013flow}, the current Haskell stream fusion mechanism introduces overhead in terms of a large number of duplicate loop counters, which increases register pressure unnecessarily. Hand fusing all code and compiling it with the same compiler (GCC) isolates the true cost of the various clusterings from low level differences in code generation.

We have used both GLPK and CPLEX as external ILP solvers. For small programs such as @normalizeInc@, both solvers produce solutions in under 100ms. For a larger randomly generated example with twenty-five combinators, GLPK took over twenty minutes to produce a solution while the commercial CPLEX solver was able to produce a solution in under one second --- which is still quite usable. We will investigate the reason for this wide range in performance in future work.

The benchmark programs are at \url{https://github.com/amosr/papers/tree/master/2014betterfusionforfilters/benches}.


% -----------------------------------------------------------------------------
\pagebreak
\subsection{Normalize2}
To demonstrate the ILP formulation we will use the @normalize2@ example from \S\ref{s:Introduction}, repeated here:
\begin{code}
  normalize2 :: Array Int -> Array Int
  normalize2 xs
   = let sum1 = fold   (+)  0   xs
         gts  = filter (>   0)  xs
         sum2 = fold   (+)  0   gts
         ys1  = map    (/ sum1) xs
         ys2  = map    (/ sum2) xs
     in (ys1, ys2)
\end{code}

We use the final ILP formulation from \S\ref{s:OptimisedConstraints}. First, we calculate $possible$ -- that is, the nodes which have no fusion-preventing path between them.
\[ \{ \{sum1, gts, sum2\}
 , \{sum1, ys2\}
 , \{gts, sum2, ys1\}
 , \{ys1, ys2\} \} \]

The complete ILP program is shown below. Note that in the objective function the weights for $x_{sum1, sum2}$ and $x_{sum2, ys1}$ are both only 1, because they do not share any input arrays.

\begin{tabbing}
MMMMM   \= MMMMMMM \= M \= MMMMMMM \= M \= MMMMMMM \= \kill
Minimise   \> $25 \cdot x_{sum1, gts} + 1  \cdot x_{sum1,sum2} + 25 \cdot x_{sum1, ys2} +$ \\
           \> $25 \cdot x_{gts, sum2} + 25 \cdot x_{gts, ys1} + 1 \cdot x_{sum2, ys1} +$ \\
           \> $25 \cdot x_{ys1, ys2}  + 5  \cdot c_{gts} + 5 \cdot c_{ys1} + 5 \cdot c_{ys2} $
\\[0.5ex]
Subject to 
    \> $-5 \cdot x_{sum1, gts}$  \> $\le$ \> $\pi_{gts} - \pi_{sum1}$  \> $\le$ \> $5 \cdot x_{sum1, gts}$  \\
    \> $-5 \cdot x_{sum1, sum2}$ \> $\le$ \> $\pi_{sum2} - \pi_{sum1}$ \> $\le$ \> $5 \cdot x_{sum1, sum2}$ \\
    \> $-5 \cdot x_{sum1, ys2 }$ \> $\le$ \> $\pi_{ys2 } - \pi_{sum1}$ \> $\le$ \> $5 \cdot x_{sum1, ys2 }$ \\
    \> $-5 \cdot x_{gts,  ys1 }$ \> $\le$ \> $\pi_{ys1 } - \pi_{gts }$ \> $\le$ \> $5 \cdot x_{gts, ys1  }$ \\
    \> $-5 \cdot x_{sum2, ys1 }$ \> $\le$ \> $\pi_{ys1 } - \pi_{sum2}$ \> $\le$ \> $5 \cdot x_{sum2, ys1 }$ \\
    \> $-5 \cdot x_{ys1, ys2  }$ \> $\le$ \> $\pi_{ys2 } - \pi_{ys1 }$ \> $\le$ \> $5 \cdot x_{ys1, ys2  }$ 
\\[0.5ex]
    \> $   x_{gts, sum2 }$ \> $\le$ \> $\pi_{sum2} - \pi_{gts }$ \> $\le$ \> $5 \cdot x_{gts, sum2 }$ 
\\[0.5ex]
    \>                     \>       \> $\pi_{sum1} < \pi_{ys1}$ \\
    \>                     \>       \> $\pi_{sum2} < \pi_{ys2}$
\\[0.5ex]
    \> $ x_{gts,sum2} $    \> $\le$ \> $c_{gts}$
\\[0.5ex]
    \> $x_{gts, sum2}$     \> $\le$ \> $x_{sum1, sum2}$ \\
    \> $x_{sum1,sum1}$     \> $\le$ \> $x_{sum1, sum2}$ \\
    \> $x_{sum1, gts}$     \> $\le$ \> $x_{sum1, sum2}$
\end{tabbing}
\noindent

One minimal solution to this is:
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$x_{sum1, gts},~ x_{sum1, sum1},~ x_{sum1, sum2},~ x_{gts, sum2},~ x_{ys1,  ys2}$
    \> $=$ \> $0$ \\
$x_{sum1, ys2},~ x_{gts, ys1},~   x_{sum2, ys1}$
    \> $=$ \> $1$ 
\\[1ex]
$\pi_{sum1},~ \pi_{gts },~ \pi_{sum2}$
    \> $=$ \> $0$ \\
$\pi_{ys1 },~ \pi_{ys2 }$
    \> $=$ \> $1$ 
\\[1ex]
$c_{gts},~ c_{ys1},~ c_{ys2}$           
    \> $=$ \> $0$
\end{tabbing}

This minimal solution is not unique, though in this case the only other minimal solutions use different $\pi$ values, and denote the same clustering. Looking at just the non-zero variables in the objective function, the value is $25 \cdot x_{sum1,ys2} + 25 \cdot x_{gts,ys1} + 1 \cdot x_{sum2, ys1} = 51$. For illustrative purposes, note that objective function could be reduced by setting $x_{sum1,ys2} = 0$ (fusing $sum1$ and $ys1$), but this conflicts with other constraints. Since $x_{sum1, sum2} = 0$, we require $\pi_{sum1} = \pi_{sum2}$, but also \mbox{$\pi_{sum2} < \pi_{ys2}$}. These constraints cannot be satisfied, so a clustering that fused $sum1$ and $ys2$ would not also permit $sum1$ and $sum2$ to be fused.

We will now compare the clustering produced by our system, with the one implied by stream fusion. As described in \cite{lippmeier2013flow}, stream fusion cannot fuse a produced array into multiple consumers, or fuse operators that are not in a producer-consumer relationship. The corresponding values of the $x_{ij}$ variables are:
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$x_{gts, sum2}$
    \> $=$ \> $0$ \\
$x_{sum1, gts}, x_{sum1, sum2}, x_{ys1,  ys2}, x_{sum1, ys2}, x_{gts, ys1 }, x_{sum2, ys1}$
    \> $=$ \> $1$
\end{tabbing}

We can force this clustering to be applied in our integer linear program by adding the above equations as new constraints. Solving the resulting program then yields:
\begin{tabbing}
MMMMMMMMMMMMMMMMMMMMMMMMMM \= M \= \kill
$\pi_{sum1}, \pi_{gts }, \pi_{sum2}$
    \> $=$ \> $0$ \\
$\pi_{ys1 }, \pi_{ys2 }$
    \> $=$ \> $1$ \\
$c_{gts}, c_{ys1}, c_{ys2}$           
    \> $=$ \> $0$
\end{tabbing}

Note that although nodes $sum1$ and $sum2$ have equal $\pi$ values, they are not fused. Conversely, if two nodes have different $\pi$ values then they are never fused. 


% -----------------------------------------------------------------------------
\subsection{Closest Points}
The closest points benchmark is a divide-and-conquer algorithm that finds the closest pair of 2-dimensional points in an array. We first find the midpoint along the Y-axis, and filter the remaining points to those above and below the midpoint. We then recursively find the closest pair of points in the two halves, and merge the results. As the filtered points are passed directly to the recursive call, there is no further opportunity to fuse them, and our clustering is the same as returned by Megiddo's algorithm. However, our clustering generates both filtered arrays in a single loop, unlike stream fusion that requires a separate loop for each.


% -----------------------------------------------------------------------------
\subsection{QuadTree}
The QuadTree benchmark recursively builds a 2-dimensional space partitioning tree from an array of points. At each step the array of points is filtered into four 2-dimensional boxes. As with the closest points algorith, there are no further opportunities for fusing the filtered results, and our clustering is the same as Megiddo's. However, our clustering produces all four filtered results in a single loop, whereas stream fusion requires four loops.


% -----------------------------------------------------------------------------
\subsection{QuickHull}
The core of the QuickHull algorithm is shown below: given a line and an array of points, we filter the points to those above the line, and also find the point farthest from that line.

\begin{code}
hull :: (Point,Point) -> Array Point -> Array Point
hull line@(l,r) pts
 = let pts' = filter (above   line) pts
       ma   = fold   (maxFrom line) pts'
   in (hull (l, ma) pts') ++ (hull (ma, r) pts')
\end{code}

Stream fusion cannot fuse the @pts'@ and @ma@ bindings because @pts'@ is referred to multiple times and thus cannot be inlined. Megiddo's algorithm also cannot fuse the two bindings because their iteration sizes are different. If the @ma@ binding was rewritten to operate over the @pts@ array instead of @pts'@, Megiddo's formulation would be able to fuse the two, and the overall program would give the same result. However, this performance behavior is counter intuitive because @pts'@ is likely to be smaller than @pts@, so in an unfused program the original version would be faster. Our system fuses both versions.

