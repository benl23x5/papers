%!TEX root = ../Main.tex
\section{Related Work}
Our work is embodied in Repa Flow, which is available on Hackage. Repa Flow is a new layer on top of the existing Repa library for delayed arrays~\cite{Lippmeier:Guiding}, and performs fusion via the GHC simplifier rather than using a custom program transformation based on series expressions~\cite{Lippmeier:DataFlow} as in our prior work.

Our system ensures that programs run in constant space, without requiring buffering or backpressure as in Akka~\cite{github:akka} and Heron~\cite{Kulkarn:Heron}. Our stream programs are fused into nested loops that read the input data from the source files, process it, and write the results immediately without blocking. The only intermediate space required is for aggregators --- for example if we had a stream of text and were counting the number of occurrences of each word we would need a map of words to the number of occurrences. 

We use stream processing to deal with large data sets that do not fit in memory. Real time streaming applications, such as to process click streams generated from websites, are the domain of synchronous data flow languages such as Lucy-n~\cite{Mandel:Lucy}, and reactive stream processing systems such as Heron~\cite{Kulkarn:Heron} and S4~\cite{Neumeyer:S4}. Lucy uses a clock analysis to determine where (finite) buffers must be introduced into the data flow graph. Heron and S4 use fixed size buffers with runtime back-pressure to match differing rates of production and consumption.

The main difference between Repa Flow and Iteratee based Haskell libraries 
\cite{Kiselyov:iteratee, hackage:enumerator, hackage:conduit, hackage:pipes} is that Repa Flow uses the separate @Sources@ and @Sinks@ types to express the \emph{endpoints} of flows, whereas an Iteratee is better thought of as a \emph{computation} as it is given a monadic interface. The advantage of the @Iteratee@ approach is that pleasing algebraic identities arise between iteratee computations. The disadvantage is that consuming data from two separate sources is awkward because each source is represented by its own monadic computation, and multiple computations must be layered using monad transformers. Repa Flow lacks the convenience of a uniform monadic interface, though writing programs that deal with many sources and sinks is straightforward by design.

The idea that parallelism can be introduced into a data flow graph via a single operator is well known in the databases community. The Volcano~\cite{Graefe:Volcano} parallel database inserts an @exchange@ operator into its query plans, which forks a child thread for the producer of some data, leaving the master thread as the consumer. The implementation of @exchange@ also introduces buffering and uses back pressure to handle mismatch between rates of production and consumption. In Repa Flow we use @drainP@ to introduce parallelism, and @drainP@ itself introduces no extra buffering. In Volcano and other database systems, communication between operators is performed with a uniform @open@, @next@, @close@ interface, similar to a streaming file API. In Repa Flow the API between operators consists of the @Sources@ and @Sinks@ type, where the next element in a given stream can be uniformly acquired via the @pull@ function.

In itself the duality between source and sink, push and pull, is folklore, and has previously been used for code generation in array processing languages~\cite{Claessen:ExpressiveArray,Svensson:Defunctionalizing} and XML processing pipelines~\cite{Kay:YouPull}. More recently, Bernardy and Svenningsson describe a library~\cite{Bernardy:Duality} that defines streams with sources and sinks, where each is defined as if it were the logical negation of the other. They also define co-sources and co-sinks, where a co-source is a sink that accepts element consumers and a co-sink is a source that produces element consumers. In related work Bernardy~\emph{et al} describe a core calculus \cite{Bernardy:Composable} based on Linear Logic which guarantees fusion does not increase the cost of program execution. The system is based fundamentally around linear logic rather than lambda calculus, with evaluation being driven by cut elimiation rather than function application. They describe a compiler targeting C and encouraging benchmark results.

