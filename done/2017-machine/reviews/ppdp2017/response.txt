
For Review 2
~~~~~~~~~~~~

> The fusion algorithm presented in this paper is similar to the
> algorithm presented in the paper Filter Fusion [1]. This is the
> biggest weakness of the present paper; the absence of a comparison
> with Filter Fusion. Although there are plenty of superficial
> differences between the two algorithms there are also some striking
> similarities. For instance both algorithms operate pairwise on two
> programs which are described as control flow graphs. I haven't made
> an in-depth comparison between the algorithms but I suspect that
> machine fusion might be somewhat stronger.

Thankyou for the reminder of Filter Fusion [1]. We were aware of this work
but had not mentioned it in the current paper as our focus was on handling
dataflow graphs with arbitrary splits and joins. As stated in the initial
paragraph of [1] "Filters [of filter fusion] read data from a single source
and write data to a single destination." The algorithm in [1] is based around
the assumption that the network to be fused is a linear pipeline, and all
the examples have this property. In contrast, our system handles the more
general case where the dataflow graph contains arbitrary splits and joins.

The main algorithmic difference between Machine Fusion and Filter Fusion
is that the former interleaves individual instructions from the various
producers and consumers rather than larger blocks of instructions at a time.
The operational semantics we provide in S3 formalizes how non-determinism
in the instruction order allows Machine Fusion to search for an interleaving
that does not require unbounded buffering. Fine grained interleaving of
instructions is needed when multiple producers feed into a single consumer
as the code for the consumer may need to accept an element from one particular
producer before another. This aspect of Machine Fusion is described in S4.1.

With that said we agree that Machine Fusion and Filter Fusion have
similarities and would be happy to clarify the difference in the related
work section of our paper.

[1] Filter Fusion. Todd A. Proebsting and Scott A. Watterson. POPL'96


> Another thing that I find rather unsatisfactory with machine fusion
> is that it is so hard to predict if fusion succeeds or not. Many
> fusion frameworks have a safe characterization which guarantees fusion.
> In shortcut fusion there are Good Consumers and Good Producers and
> whenever they are composed we know that the intermediate structure
> will be removed. What good is the use of all the power of machine
> fusion, if I don't know if my program will fuse in the end?

As stated in S4.1, "For fusion of pipelines of standard combinators such
as map, fold, filter, scan and so on, fusion always succeeds." This
property arises naturally from the structure of our system. For general
dataflow networks our safe characterisation is simply "if the process
network cannot deadlock then fusion always succeeds". In general the
analysis of whether the network can actually deadlock is up to the user,
as we provide a language where they can write aribrary processes. Linear
pipelines such as those handled by build/foldr fusion cannot deadlock,
so we can always fuse those.


For Review 1
~~~~~~~~~~~~~

> As a result, the correctness of the stream combinators seems like it
> is still open. For example, how could we prove that the machine
> implementations in Figure 2 correspond to the naive recursive
> implementation of stream functions a Haskell programmer like myself
> would write? In particular, why should we believe that the translation
> of a functional program is deterministic (as the naive stream program
> would be)?

As with Kahn Process Networks, all communication is via blocking reads,
so the overall computation is deterministic (S3.1). This makes it possible
to reason about evaluation of a single process in isolation, and apply that
reasoning to an entire network.


> * One technical question is that the fusePair function in Figure 10
>   has the type
>
>     fusePair : Process → Process → Maybe Process
>
>   But the Coq theorem quoted in the paper reads :
>
>     Soundness : [...] EvalBs (fuse P1 P2) ss h (LX l1 l2 is1 is2) -> ...
>
>   Here, the call "fuse P1 P2" seems like it has to be a process, rather
>   than a Maybe Process.
>

This is an artefact of our encoding in Coq. The described fusion algorithm
invents fresh labels during a fixpoint computation and it's hard to convice
Coq that this terminates. (The upper bound is the product of number of states
in each machine and the number of channels and channel states)

Instead, we define the label type as either a valid label (LX) from each
process, or invalid (L'INVALID). If fusion fails the result code jumps to
the invalid label. This allows us to define a total function from label
to instruction instead of really computing the fixpoint. The soundness theorem
states that if the result process reaches a valid label then you have a valid
evaluation of the unfused processes.

