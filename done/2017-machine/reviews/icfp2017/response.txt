
> Is the extended class of examples that the new system can handle of practical importance in real programming?

Practical examples include:

partitioning (filter f xs, filter (not.f) xs), which is the core of quickselect: the fact that vector includes a hand-fused version indicates its utility, but even the handfused version will not fuse with consumers;
filterMax (filter (>0) xs, maximum xs) which is the core of quickhull;
original motivation was vectorised code as generated by Data Parallel Haskell, which introduces large replicated arrays. If the replicated arrays are not fused away, the space complexity of the program is changed quadratically. Shortcut, or single producer-consumer, fusion cannot guarantee the removal of these arrays.

> How would uniquesUnion behave in a lazy language?

This is the same problem as unzip in lazy evaluation, addressed in our previous paper 'Data Flow Fusion with Series Expressions' page 12.

https://www.cse.unsw.edu.au/~chak/papers/flow-fusion.pdf

If one wanted to write the outputs to a file, one would probably loop over sUnique writing it to a file, then do the same for sUnion. You might do them in the other order, but either way it would need to buffer the entire input until you read the other stream. We really need the fused process to be in control of when to write each element to each file - it should be pushing the outputs rather than being pulled on.

> How are source programs translated into the process language?

Combinators are written as processes, embedded in the source language. Users then use the source language to construct a graph of combinators, except at a meta-level (eg Template Haskell). Processes are then fused, and translated back to the source language for code generation.

> Why…an imperative intermediate language?

The process language is analogous to the restrictions placed on step functions in stream fusion: they must be non-recursive with finite states in order for SpecConstr to reliably fire.

> What are the key ideas…?

Shortcut fusion techniques rely on inlining to move a producer into a consumer, but this only works when there is a single consumer; otherwise work would be duplicated. By reifying the combinator graph, the fusion algorithm is able to perform controlled inlining such that work is not duplicated, but producers can be used multiple times.

> How can the new approach be understood from a mathematical perspective?

The core mathematical idea is synchronised product, but this causes artificial deadlocks when both processes read from the same two inputs in different orders. Instead, we allow some reordering by adding a single buffer element's leeway between when the fused process reads, and when the original process reads. This concept of reordering is yet to be formalised, however. One could think of it as synchronised product with a tiny bit extra; synchronised product alone is still more powerful than shortcut fusion, since shortcut fusion does not allow multiple consumers, synchronised product will not deadlock in shortcut fusion cases.

> Is there a system that can be downloaded and experimented with…?

There is a raw, early Haskell implementation here: https://github.com/amosr/folderol
