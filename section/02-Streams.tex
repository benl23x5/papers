%!TEX root = ../Main.tex

\clearpage
% ---------------------------------------------------------
\section{Streams and Flows}

A \emph{stream} is an array of elements where the indexing dimension is time. As each element is read from the stream it is available only in that moment, and if the consumer wants to re-use the element at a later time it must save it itself. A \emph{flow} is a bundle of related streams, where each stream typically carries data from a single partition of a larger data set --- we might create a flow consisting of 8 streams where each stream carries data from a 1GB partition of a 8GB data set.

In our API we manipulate flow endpoints rather than the flows themselves. We use the following data types:

\begin{code}
data Sources i m e 
   = Sources
   { arity :: i
   , pull  :: i -> (e -> m ()) -> m () -> m () }

data Sinks   i m e 
   = Sinks   
   { arity :: i
   , push  :: i -> e -> m ()
   , eject :: i -> m () }
\end{code}

Type @Sources i m e@ classifies flow sources which can produce elements of type @e@, using some monad @m@, where the individual streams in the bundle are indexed by values of type @i@. Likewise @Sinks i m e@ classifies flow sinks which can consume elements of type @e@.

In the @Sources@ type, field @arity@ stores the number of individual streams in the bundle. To receive data from a flow producer we use the function in the @pull@ field, passing the index of type @i@ for the desired stream, an \emph{eat} function of type @(e -> m ())@ to consume an element if one is available, and an \emph{eject} function of type @(m ())@ which will be called when no more elements will ever be available for the specified stream. The pull function will then perform an @(m ())@ computation before either calling our eat or eject function, depending on whether data is available from the source.

In the @Sinks type@, field @arity@ stores the number of individual streams as before. To send data to a flow consumer we call the @push@ function, passing the stream index of type @i@, and the element of type @e@. The @push@ function will then perform an @(m ())@ computation to consume the provided element. If no more data is available we instead call the @eject@ function, passing the stream index of type @i@, and this function will perform an @(m ())@ computation to shut down the flow sink --- possibly closing files or disconnecting network sockets.

Consuming data from a @Sources@ and producing data to a @Sinks@ is synchronous, meaning that at runtime the computation will block until either an is produced or no more elements are available (when consuming); or an element is consumed or the endpoint is shut down (when producing). Finally, note that the @eject@ functions used in both the @Sources@ and @Sinks@ type are associated with a single stream only. If our flow consists of 8 streams attached to 8 separate files then ejecting a single stream will close a single file.

\subsection{Sourcing, Sinking and Draining}
Figure~\ref{f:Draining} gives the definitions of @sourceFs@, @sinkFs@ which create flow sources and sinks based on a list of files, as well as @drainP@ which pulls data from a flow source and pushes it to a sink. 

Given the definition of the @Sources@ and @Sinks@ types, writing @sourceFs@ and @sinkFs@ is straightforward. In @sourceFs@ we first open all the provided files, yielding file handles for each one, and the @pull@ function for each stream source reads data from the corresponding file handle. When we reach the end of one of the files we eject the correponding stream. In @sinkFs@ the @push@ function from the stream simply writes the provded element to the corredponding file, and the @eject@ function closes it.

The @drainP@ function takes a bundle of stream @Sources@, a bundle of stream @Sinks@, and drains all the data from each source into the corresponding sink. Importantly, @drainP@ forks a separate thread to drain each stream, making the system data parallel. We use intermediate Haskell MVars to communicate between the worker threads and the main threads, so after forking all the workers the main threads waits until they are all finished. Now that we have the @drainP@ function, we can copy a partitioned data set from one set of files to another, in parallel:

\begin{code}
 copySetP :: [FilePath] -> [FilePath] -> IO ()
 copySetP srcs dsts
  = do  ss <- sourceFs srcs
        sk <- sinkFs   dsts
        drainP ss sk
\end{code}


\begin{figure}
\begin{code}
sourceFs :: [FilePath] -> IO (Sources Int IO Char)
sourceFs names = do
 hs <- mapM (\n -> openFile n ReadMode) names
 let pulls i ieat ieject
      = do let h = hs !! i
           eof <- hIsEOF h
           if eof then hClose   h >> ieject
                  else hGetChar h >>= ieat
 return (Sources (length names) pulls)

sinkFs  :: [FilePath] -> IO (Sinks Int IO Char)
sinkFs names = do
 hs <- mapM (\n -> openFile n WriteMode) names
 let pushs  i e = hPutChar (hs !! i) e
 let ejects i   = hClose   (hs !! i)
 return (Sinks (length names) pushs ejects)

drainP :: Sources Int IO a -> Sinks Int IO a -> IO ()
drainP (Sources i1 ipull) 
       (Sinks   i2 opush oeject) = do
 let drainStream i
      = ipull i eats ejects 
      where eats   v = opush  i v >> drainStream i
            ejects   = oeject i
 let makeDrainer i = do
        mv <- newEmptyMVar 
        forkFinally (drainStream i) 
                    (\_ -> putMVar mv ())
        return mv 
 mvs <- mapM makeDrainer [0 .. min i1 i2]
 mapM_ readMVar mvs
\end{code}

\caption{Sourcing, Sinking and Draining}
\label{f:Draining}
\end{figure}


% ---------------------------------------------------------
\subsection{Stateful Streams, Branching and Linearity}

Suppose we wish to copy our set of files to \emph{two} different destination sets instead of a single one. Trying to drain the source flow into two separate sinks does not work, which is a key part of the design of our system. 

\eject 
We might try something like:

\begin{code}
 badCopyMultiple 
  :: [FilePath] -> [FilePath] -> [FilePath] -> IO ()
 badCopyMultiple srcs dsts1 dsts2
  = do  ss  <- sourceFs srcs
        sk1 <- sinksFs  dsts1
        sk2 <- sinksFs  dsts2
        drainP ss sk1
        drainP ss sk2
\end{code}

This cannot work, nor should it. The @Sinks@ endpoint created by @sourceFs@ is a stateful object -- it represents the current position in each of the source files being read. After we have applied the first @drainP@, we have already finished reading through all the source files, so draining the associated flow again does not yield more data. 

Each of the drain computations is performed in the IO monad, which enforces an explicit notion of sequence. The second drain happens after the first one. To copy the source data using two separate drain stages we would need to either rewind the file handles attached to the sources, or buffer the data read from the files in memory. Both options are bad. An object of type @Sources@ is an abstract producer of data, and in general it may not be possible to rewind it to a previous state --- suppose it was connected to a stream of sensor readings. On the other hand, buffering all the data read from the flow source so that we can write it one set of files, then another, does not work when the source contains more data than will fit in memory. Instead, we introduce an explicit combinator which branches our flow:

\begin{code}
dup_ooo :: (Ord i, Monad m)
       => Sinks i m a -> Sinks i m a -> Sinks i m a
dup_ooo (Sinks n1 push1 eject1) 
        (Sinks n2 push2 eject2)
 = let pushs  i x = push1 i x >> push2 i x
       ejects i   = eject1 i  >> eject2 i
   in  Sinks (min n1 n2) pushs ejects
\end{code}

The @dup_ooo@ combinator takes two separate sinks and creates a new one. When we push an element to the new sink it will push that element to the two original sinks. Likewise, when we eject the new sink it will eject the two original sinks. We can use this new combinator to write a working @copyMultiple@ function:

\begin{code}
copyMultipleP 
 :: [FilePath] -> [FilePath] -> [FilePath] -> IO ()
copyMultipleP srcs dsts1 dsts2
 = do  ss  <- sourceFs srcs
       sk1 <- sinkFs   dsts1
       sk2 <- sinkFs   dsts2 
       drainP ss (dup_oo sk1 sk2)
\end{code}

This new @copyMultiple@ function runs in constant space, which is the behaviour we wanted. Importantly, note that in the new function definition there is only a single occurrence of each of the variables bound to sources and sinks: @ss@, @sk1@, @sk2@. Each source and sink is used linearly. Our program expresses a data flow graph, where the functions @sourceFs@, @sinkFs@ and @dup_ooo@ create nodes in the graph, and the use-def relation of variables connects the nodes with edges.


% ---------------------------------------------------------
\subsection{Polarity and buffering}
In the previous section, @dup_ooo@ branches a flow by taking two existing sinks and produces a new one. The @_ooo@ suffix stands for ``output, output, output'', referring to the three sinks. It turns out that the converse @dup_iii@ combinator is not implementable without requiring unbounded buffering. Such a combinator would have the following type:
\begin{code}
 dup_iii :: (Ord i, Monad m)
         =>  Sources i m a 
         -> (Sources i m a, Source i m a)
\end{code}

Consider how this would work. The @dup_iii@ combinator takes an argument source and produces two result sources. Now suppose we pull data from the left result source. The combinator would need to pull from its argument source to retrieve this data, then when we pull from the right result source we want the same data. The problem is that there is nothing stopping us from pulling the entire stream via the left result source before pulling any elements from the right result source, so @dup_iii@ would need to introduce an unbounded buffer to store all the elements in the interim.

Interestingly, although @dup_iii@ cannot work without an unbounded buffer, a hybrid combinator @dup_ioi@ can. This combinator has the following definition:
\begin{code}
dup_ioi :: (Ord i, Monad m)
        => Sources i m a -> Sinks i m a 
        -> Sources i m a
dup_ioi (Sources n1 pull1) (Sinks n2 push2 eject2)
 = let pull3 i eat3 eject3
        = pull1 i eat1 eject1
        where eat1   x = eat3 x >> push2  i x
              eject1   = eject3 >> eject2 i
   in  Sources (min n1 n2) pull3
\end{code}

The @dup_ioi@ combinator takes an argument flow source, an argument flow sink, and returns a result flow source. When we pull data from the result source the combiantor pulls from its argument source and then \emph{pushes} the same data to the argument sink. Similarly to @dup_ooo@, we can use @dup_ioi@ to introduce a branch into the data flow graph \emph{without} requiring unbounded buffering of the input flow. Actually, the same approach works for any number of argument sinks, for example:
\begin{code}
dup_iooi
 :: (Ord i, Monad m)
 => Sources i m a -> Sinks i m a -> Sinks i m a
 -> Sources i m a
\end{code}

With @dup_iooi@ when we pull from the result source the combinators pulls from its argument source and pushes the same data to its argument sinks. 

Figure~\ref{f:Polarity} the various options for assigning polarities to the argument and result endpoints of a flow duplication combinator. Sources are indicated with a $\bullet$ and Sinks with a $\circ$. We use the mnemonic that the filled $\bullet$ can always produce data (being a source) while the empty $\circ$ can always accept data (being a sink). In the figure the greyed out diagrams are for polarity assignments that would require unbounded buffering, while the solid ones require no buffering.

\begin{figure}
\includegraphics[scale=0.7]{figures/polarity.pdf}

\caption{Possible polarities for the flow duplication combinator}
\label{f:Polarity}
\end{figure}

In the right of Figure~\ref{f:Polarity} we have also included the corresponding polarity diagram for the @drainP@ combinator from Figure~\ref{f:Draining}. The @drainP@ combinator requires no buffering because all data pulled from the source is immediately pushed to the sink. On the other hand, if we invert the polarities of @drainP@ we arrive at the natural assignment for a primitive buffering combinator. A combinator that consumes data via a single argument sink and produces the same data via a result source \emph{must} introduce a buffer as there is no guarantee that new elements are pushed to the sink at the same rate they are pulled from the source. 

Repa library makes it easy to guarantee that data flow programs written with it run in constant space, by only providing combinators with the polarity assignments that do so. This guarantee also requires that variables bound to sources and sinks are used linearly, which alas we cannot check with Haskell as it does not support linear types. However, linearity is an easy constraint to understand and check manually, so for now we we leave the job of enforcing it to the client programmer.

In future work would be interesting to implement a system to take a data flow graph without a polarity assignment, and assign polarities to the combinators such that the entire graph can be executed without buffering (if possible). However, so far the largest programs we have written using the library have included only 10-15 combinators in a single function, and assigning the polarities has not been a burden. In practice there is usually a natural distinction between the \emph{input network} written using sources and the \emph{output network} using sinks, and hybrid combinators such as @dup_iio@ are used infrequently.


% ---------------------------------------------------------
\subsection{Mapping, Folding and Grouping}
Map and co-map, folding, grouping and other basic combinators. Given an example that folds the input stream while passing it through to another combinator.

\subsection{Stream projection and funneling}

\begin
{figure*}
\begin{code}
-- Conversion
fromList   :: i -> [a] -> m (Sources i m a)
toList1    :: i -> Sources i m a -> m [a]

fromLists  :: [[a]] -> m (Sources Int m a)
toLists    :: Sources Int m a -> m [[a]]

-- Computation
drainS     :: Sources i   m  a -> Sinks i   m  a -> m  ()
drainP     :: Sources Int IO a -> Sinks Int IO a -> IO ()

-- Mapping
map_i      :: (a -> b) -> Sources i m a  -> m (Sources i m b)
map_o      :: (b -> a) -> Sinks   i m b  -> m (Sinks   i m a)

zipWith_ii ::  (a -> b -> c) 
                  -> Sources i m a -> Sources i m b -> m (Sources i m c)
zipWith_io :: ... -> Sources i m a -> Sinks   i m c -> m (Sinks   i m b)
zipWith_oi :: ... -> Sinks   i m a -> Sources i m b -> m (Sinks   i m a)

-- Connection
dup_oo     ::        Sinks   i m a -> Sinks   i m a -> m (Sinks   i m a)
dup_io     ::        Sources i m a -> Sinks   i m a -> m (Sinks   i m a)
dup_oi     ::        Sinks   i m a -> Sources i m a -> m (Sinks   i m a)

connect_i  ::        Sources i m a -> m (Sources i m a, Sources i m a)

-- Projection
project_i  :: i ->   Sources i m a -> m (Sources () m a)
project_o  :: i ->   Sinks   i m a -> m (Sinks   () m a)

-- Funneling
funnel_i   ::        Sources i m a -> m (Sources () m a)
funnel_o   ::        Sinks  () m a -> m (Sinks   i  m a)

-- Elided constraints: (Monad m, States i m) => ...
\end{code}
\caption{Generic Flow operators}
\end{figure*}


